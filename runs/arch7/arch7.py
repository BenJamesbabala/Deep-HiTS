def convNetArchitecture(im_chan, batch_size, im_size, activation):
    arch_def = [{"layer": "DataLayer",
                 "im_chan": im_chan,
                 "batch_size": batch_size,
                 "im_size": im_size},
                {"layer": "CreateRotationsLayer"},
                {"layer": "ConvPoolLayer",
                 "num_output": 32,
                 "filter_size": 4,
                 "pad": 3,
                 "pool_size": 1,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "ConvPoolLayer",
                 "num_output": 32,
                 "filter_size": 3,
                 "pad": 1,
                 "pool_size": 2,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "ConvPoolLayer",
                 "num_output": 64,
                 "filter_size": 3,
                 "pad": 1,
                 "pool_size": 1,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "ConvPoolLayer",
                 "num_output": 64,
                 "filter_size": 3,
                 "pad": 1,
                 "pool_size": 1,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "ConvPoolLayer",
                 "num_output": 64,
                 "filter_size": 3,
                 "pad": 1,
                 "pool_size": 2,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "JoinRotationsLayer"},
                {"layer": "HiddenLayer",
                 "num_output": 64,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "DropoutLayer",
                 "p_drop": 0.5},
                {"layer": "HiddenLayer",
                 "num_output": 64,
                 "activation": activation,
                 "init_type": "ReLU"},
                {"layer": "DropoutLayer",
                 "p_drop": 0.5},
                {"layer": "LogisticRegression",
                 "num_output": 2,
                 "init_type": "ReLU"}]
    return arch_def
